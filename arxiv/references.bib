@article{eckart1936approximation,
  title={The approximation of one matrix by another of lower rank},
  author={Eckart, Carl and Young, Gale},
  journal={Psychometrika},
  volume={1},
  number={3},
  pages={211--218},
  year={1936},
  publisher={Springer}
}

@article{halko2011finding,
  title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions},
  author={Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A},
  journal={SIAM review},
  volume={53},
  number={2},
  pages={217--288},
  year={2011},
  publisher={SIAM}
}

@article{wang2020hat,
  title={Hat: Hardware-aware transformers for efficient natural language processing},
  author={Wang, Hanrui and Zhang, Zhangyang and Liu, Shiyu and Guo, J \'Onathan and Zhang, Xiangyu and Zhang, Zhe and Carin, Laurent},
  journal={arXiv preprint arXiv:2005.14187},
  year={2020}
}

@article{nvidia2017tensor,
  title={NVIDIA Tensor Core Programmability},
  author={NVIDIA Corporation},
  journal={White Paper},
  year={2017}
}

@article{micikevicius2022fp8,
  title={FP8 Formats for Deep Learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@article{nvidia2018cublas,
  title={cuBLAS Library},
  author={NVIDIA Corporation},
  journal={NVIDIA Developer Documentation},
  year={2018}
}

@article{chetlur2014cudnn,
  title={cuDNN: Efficient primitives for deep learning},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  year={2014}
}

@article{zhu2018mixed,
  title={Mixed precision training},
  author={Zhu, Hao and Prabhu, Sashank and Huang, Xiaodong and Xiong, Wenhua and Liu, Chao and Zhang, Tong and Liu, Juncheng and Zhu, Yu and Li, Dianhai},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{gupta2015deep,
  title={Deep learning with limited numerical precision},
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  journal={International Conference on Machine Learning (ICML)},
  year={2015}
}

@article{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{nvidia2023cuda,
  title={CUDA C++ Programming Guide},
  author={NVIDIA Corporation},
  journal={NVIDIA Developer Documentation},
  year={2023}
}

@article{jia2018efficient,
  title={Efficient sparse GEMM on GPUs},
  author={Jia, Zhihao and Maggioni, Marco and Smith, Jeffrey and Austern, Mauricio and Isard, Michael},
  journal={International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  year={2018}
}

@article{smith2022using,
  title={Using {D}eep {S}peed and {M}egatron to {T}rain {M}egatron-{T}uring {NLG} 530{B}},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{dongarra1990linpack,
  title={LINPACK Users' Guide},
  author={Dongarra, Jack J and Bunch, James R and Moler, Cleve B and Stewart, Gilbert W},
  journal={Society for Industrial and Applied Mathematics},
  year={1979}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Lukasz and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{shoemaker2019mega,
  title={Mega-scale deep learning for speech recognition},
  author={Shoemaker, David},
  journal={arXiv preprint arXiv:1904.08779},
  year={2019}
}

@article{borm2003hierarchical,
  title={Hierarchical matrices},
  author={B{\"o}rm, Steffen},
  journal={Lecture Notes},
  year={2003}
}

@article{martinsson2011randomized,
  title={A randomized algorithm for the decomposition of matrices},
  author={Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
  journal={Applied and Computational Harmonic Analysis},
  volume={30},
  number={1},
  pages={47--68},
  year={2011},
  publisher={Elsevier}
}

@article{landa2023low,
  title={Low-rank adaptation of large language models},
  author={Landa, Joel and Kolter, J Zico and Li, David},
  journal={arXiv preprint arXiv:2309.04530},
  year={2023}
}

@article{dettmers2022llm,
  title={LLM.int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{nvidia2021cutlass,
  title={CUTLASS: Fast linear algebra in CUDA C++},
  author={NVIDIA Corporation},
  journal={NVIDIA Developer Blog},
  year={2021}
}

@article{tillet2021triton,
  title={Triton: An intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, H T and Cox, David},
  journal={Proceedings of the 4th ACM SIGPLAN International Symposium on Machine Programming},
  pages={10--19},
  year={2021}
}

@article{hazy2023matrix,
  title={Matrix multiplication with reduced precision},
  author={Hazy, Yitzhak and Schwartz, Rotem and Finkelstein, Naama and Schwartz, Oded},
  journal={arXiv preprint arXiv:2309.14021},
  year={2023}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{bradbury2018jax,
  title={JAX: composable transformations of Python+NumPy programs},
  author={Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and Vander{\textasciicaron}Plas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  journal={GitHub},
  year={2018}
}

@article{wang2019fusedmm,
  title={FusedMM: A unified SDDMM-SpMM kernel for graph embedding and inference},
  author={Wang, Yaojun and Li, Li and Zhang, Zheng and He, Mingxing and Huang, Guangyu and Wang, Cheng and Zhang, Wei and Lin, Haifeng},
  journal={arXiv preprint arXiv:1910.03158},
  year={2019}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{openai2023gpt4,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{yao2022smoothquant,
  title={SmoothQuant: Accurate and efficient post-training quantization for large language models},
  author={Yao, Guangxuan and Wu, Yingwei and Dai, Xinyu and Li, Yujie and Zhang, Peng and Wang, Yuxiang and Zhang, Yu},
  journal={International Conference on Machine Learning (ICML)},
  pages={38087--38099},
  year={2022}
}

@article{lin2023awq,
  title={AWQ: Activation-aware weight quantization for LLM compression and acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}
